#######################################################################################
# evaluation.py

# Evaluates recommendation quality using:
# - BERTScore and ROUGE-L for response quality
# - User feedback (ratings from 1 to 5)

# Also logs evaluations and feedback in JSON format for future learning or fine-tuning. 
# 
# Functions:
### - collect_user_feedback
### - compute_BERTScore
### - compute_ROUGE_L
### - evaluate_recommendations
### - log_evaluation
### - is_empty_log_feedback
#######################################################################################

from typing import List, Dict
from pathlib import Path
from datetime import datetime
from bert_score import score
from rouge_score import rouge_scorer
import json

# Path to log evaluation data
EVAL_LOG = Path("recommendation_engine/evaluation_log.json")


def collect_user_feedback(recommended: Dict) -> Dict:
    """
    Ask the user to rate the selected point of interest from 1 to 5,
    and optionally explain their choice.

    Args:
        recommended (Dict): selected point of interest dictionary (with 'name')

    Returns:
        Dict: feedback containing station name, rating, and explanation
    """
    feedback = {
        "point_of_interest": recommended.get("name", "Unknown"),
        "rating": 0,
        "explanation": ""
    }

    print(f"\nYou selected: {feedback['point_of_interest']}")
    
    # Ask for rating with input validation
    while True:
        try:
            rating = int(input("Rate this station from 1 (bad) to 5 (excellent): "))
            if 1 <= rating <= 5:
                feedback["rating"] = rating
                break
            else:
                print("Please enter a number between 1 and 5.")
        except ValueError:
            print("Invalid input. Please enter a number.")

    # Explanation
    explanation = input("Please explain your rating: ").strip()
    feedback["explanation"] = explanation if explanation else None

    return feedback

def compute_BERTScore(generated_response: str) -> float:
    """
    Compute the BERTScore (F1) of the generated response compared to a generic reference structure.

    Args:
        generated_response (str): The response generated by the model.

    Returns:
        float: The BERTScore F1 score (higher is better).
    """
    abstract_reference = (
        "Here are a few nearby places:\n\n"
        "• Name and distance\n"
        "• Name and distance\n\n"
        "Would you like to navigate to one of them?"
    )

    # Compute BERTScore
    _, _, F1 = score([generated_response], [abstract_reference], lang="en")

    return round(F1[0].item(), 3)


def compute_ROUGE_L(generated_response: str) -> float:
    """
    Compute the ROUGE-L (Longest Common Subsequence) score of the generated response compared to a generic reference.

    Args:
        generated_response (str): The response generated by the model.

    Returns:
        float: The ROUGE-L F1 score (higher is better).
    """

    abstract_reference = (
        "Here are a few nearby places:\n\n"
        "• Name and distance\n"
        "• Name and distance\n\n"
        "Would you like to navigate to one of them?"
    )

    # Compute ROUGE-L score
    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
    scores = scorer.score(abstract_reference, generated_response)

    return round(scores['rougeL'].fmeasure, 3)


def evaluate_recommendations(recommended: str, feedback: Dict, BERTScore: float, RougeScore: float) -> Dict:
    """
    Calculate precision and recall based on user feedback.
    A station is considered relevant if it has a rating ≥ 4.

    Args:
        recommended (str): name of the recommended point of interest
        BERTScore (float): BERTScore F1 score for the generated response
        RougeScore (float): ROUGE-L F1 score for the generated response
        feedback (Dict): POIs name → user rating (1 to 5)

    Returns:
        Dict: evaluation results with precision, recall, relevant items, and timestamp
    """

    result = {
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "recommended": recommended,
        "feedback": feedback,
        "BERTScore": round(BERTScore, 3),
        "RougeScore": round(RougeScore, 3),
    }

    log_evaluation(result)

    return result


def log_evaluation(entry: Dict):
    """
    Save the evaluation result and feedback into a log file for future analysis.

    Args:
        entry (Dict): evaluation result to store
    """
    if not EVAL_LOG.exists() or EVAL_LOG.stat().st_size == 0:
        # File doesn't exist or is empty → create and write first entry
        with EVAL_LOG.open("w") as f:
            json.dump([entry], f, indent=4)
    else:
        with EVAL_LOG.open("r+", encoding="utf-8") as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError:
                data = []

            data.append(entry)
            f.seek(0)
            json.dump(data, f, indent=4)
            f.truncate()


def is_empty_log_feedback() -> bool:
    """
    Check if the evaluation log file is empty or non-existent.

    Returns:
        bool: True if the file is empty or doesn't exist, False otherwise.
    """
    return not EVAL_LOG.exists() or EVAL_LOG.stat().st_size == 0


if __name__ == "__main__":
    generated = "There are three stations close by: Cranfield University (BP Pulse) 1.42 km, Cranfield University - Martell House 1.63 km, and Marston Vale Forest Centre at 4.99 km. Which one do you prefer?"

    # Calcul de BERTScore
    bert_score = compute_BERTScore(generated)
    print(f"BERTScore: {bert_score}")

    # Calcul de ROUGE-L
    rouge_l_score = compute_ROUGE_L(generated)
    print(f"ROUGE-L: {rouge_l_score}")

    # Example usage
    recommended = {
        "name": "Cranfield University (BP Pulse (UK))",
        "score": 4.5
    }

    _feedback = is_empty_log_feedback()
    print(f"Is feedback log empty? {_feedback}")
    feedback = collect_user_feedback(recommended)
    log_evaluation(feedback)
    # print(feedback)
